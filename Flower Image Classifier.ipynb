{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,models,transforms\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some variables declared for the helper functions below to make and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = True                              #can be True or False if you have a CUDA enabled gpu. I have NVIDIA 1060 so I have declared it as True\n",
    "arch = \"vgg\"                            #can be vgg or densenet for the purpose of this propject. I couldnt experiment with resnet or inception model due to the training time required\n",
    "lr = 0.001                              #learinng rate\n",
    "hidden_units = 100                      #number of nodes to use in each hideen layers on top of the pretrained model\n",
    "epochs = 10                             #number of iterations\n",
    "data_dir = \"flowers\"                    #PATH of the data folder \n",
    "saved_model = \"model.pth\"               #name of the checkpoint file to be saved as\n",
    "nThreads = 4                            #number of threads to use for parallel processing\n",
    "batch_size = 8                          #mini batch size for the gradient updates\n",
    "use_gpu = torch.cuda.is_available()     #to check the cuda eanbled gpu is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(data_dir):\n",
    "    data_dir = data_dir\n",
    "    train_dir = data_dir + '/train'\n",
    "    valid_dir = data_dir + '/valid'\n",
    "    test_dir = data_dir + '/test'\n",
    "\n",
    "    train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "    valid_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                          transforms.CenterCrop(224),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "    test_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                          transforms.CenterCrop(224),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "    image_datasets = dict()\n",
    "    image_datasets['train'] = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "    image_datasets['valid'] = datasets.ImageFolder(valid_dir, transform=valid_transforms)\n",
    "    image_datasets['test'] = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "    dataloaders = dict()\n",
    "    dataloaders['train'] = torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True)\n",
    "    dataloaders['valid'] = torch.utils.data.DataLoader(image_datasets['valid'], batch_size=batch_size)\n",
    "    dataloaders['test']  = torch.utils.data.DataLoader(image_datasets['test'], batch_size=batch_size)\n",
    "\n",
    "    return dataloaders, image_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for the Feedforward and Backpropagation Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model( model, criterion, optimizer, scheduler, epochs=25):\n",
    "    \n",
    "    dataloaders, image_datasets = data(data_dir)\n",
    "    if gpu:\n",
    "        if use_gpu:\n",
    "            model = model.cuda()\n",
    "            print (\"Using GPU: \"+ str(use_gpu))\n",
    "        else:\n",
    "            print(\"Using CPU since GPU is not available/configured\")\n",
    "    print_every = 50\n",
    "    steps = 0\n",
    "    for e in range(epochs):\n",
    "        current_loss = 0\n",
    "        for images,labels in dataloaders['train']:\n",
    "            steps += 1\n",
    "            images,labels = images.to('cuda'),labels.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(images)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current_loss += loss.item()\n",
    "            if steps%print_every == 0:\n",
    "                model.eval()\n",
    "                vloss = 0\n",
    "                accuracy = 0\n",
    "                for vimages,vlabels in dataloaders['valid']:\n",
    "                    optimizer.zero_grad()\n",
    "                    vimages,vlabels = vimages.to('cuda:0'),vlabels.to('cuda:0')\n",
    "                    model.to('cuda:0')\n",
    "                    with torch.no_grad():\n",
    "                        voutputs = model.forward(vimages)\n",
    "                        vloss = criterion(voutputs,vlabels)\n",
    "                        ps = torch.exp(voutputs).data\n",
    "                        eq = (vlabels == ps.max(1)[1])\n",
    "                        accuracy += eq.type_as(torch.FloatTensor()).mean()\n",
    "                vloss = vloss/len(dataloaders['valid'])\n",
    "                accuracy = accuracy/len(dataloaders['valid'])\n",
    "                print(\"epoch {}/{} :\".format(e+1,epochs),\n",
    "                        \" training loss = {}\".format(current_loss/print_every),\n",
    "                        \" validation loss = {}\".format(vloss),\n",
    "                        \" accuracy = {:.2f} %\".format(accuracy*100))\n",
    "                current_loss = 0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Wrapper for Loading Data, Building Model, Training Model and Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_wrapper(arch):\n",
    "    dataloaders, image_datasets = data(data_dir)\n",
    "    if arch == 'vgg': \n",
    "        model = models.vgg16(pretrained=True)\n",
    "    elif arch == 'densenet':\n",
    "        model = models.densenet121(pretrained=True)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_features = model.classifier[0].in_features\n",
    "    from collections import OrderedDict\n",
    "    classifier = nn.Sequential(OrderedDict([\n",
    "                              ('fc1', nn.Linear(num_features, 512)),\n",
    "                              ('relu', nn.ReLU()),\n",
    "                              ('drpot', nn.Dropout(p=0.5)),\n",
    "                              ('hidden', nn.Linear(512, hidden_units)),                       \n",
    "                              ('fc2', nn.Linear(hidden_units, 102)),\n",
    "                              ('output', nn.LogSoftmax(dim=1)),\n",
    "                              ]))\n",
    "\n",
    "    # Reserve for final layer: ('output', nn.LogSoftmax(dim=1))\n",
    "        \n",
    "    model.classifier = classifier    \n",
    "    if gpu:\n",
    "        if use_gpu:\n",
    "            model = model.cuda()\n",
    "            print (\"Using GPU: \"+ str(use_gpu))\n",
    "        else:\n",
    "            print(\"Using CPU since GPU is not available/configured\")\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    model = train_model( model, criterion, optimizer, exp_lr_scheduler,epochs=epochs)\n",
    "\n",
    "    #CHECKPOINT\n",
    "    model.class_to_idx = dataloaders['train'].dataset.class_to_idx\n",
    "    model.epochs = epochs\n",
    "    checkpoint = {'input_size': [3, 224, 224],\n",
    "                  'batch_size': dataloaders['train'].batch_size,\n",
    "                  'output_size': 102,\n",
    "                  'arch': arch,\n",
    "                  'state_dict': model.state_dict(),\n",
    "                  'optimizer_dict':optimizer.state_dict(),\n",
    "                  'class_to_idx': model.class_to_idx,\n",
    "                  'epoch': model.epochs}\n",
    "    torch.save(checkpoint, saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model using the Wrapper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: True\n",
      "Using GPU: True\n",
      "epoch 1/10 :  training loss = 4.911722822189331  validation loss = 0.0481501966714859  accuracy = 19.66 %\n",
      "epoch 1/10 :  training loss = 3.8260649919509886  validation loss = 0.04654465988278389  accuracy = 24.03 %\n",
      "epoch 1/10 :  training loss = 3.262538559436798  validation loss = 0.03038039430975914  accuracy = 40.53 %\n",
      "epoch 1/10 :  training loss = 2.924436345100403  validation loss = 0.033538028597831726  accuracy = 38.96 %\n",
      "epoch 1/10 :  training loss = 2.5339330720901487  validation loss = 0.03253711014986038  accuracy = 44.66 %\n",
      "epoch 1/10 :  training loss = 2.2668577361106874  validation loss = 0.024925580248236656  accuracy = 51.58 %\n",
      "epoch 1/10 :  training loss = 2.1839340460300445  validation loss = 0.024138793349266052  accuracy = 56.67 %\n",
      "epoch 1/10 :  training loss = 2.083612319231033  validation loss = 0.03181729465723038  accuracy = 57.04 %\n",
      "epoch 1/10 :  training loss = 1.9842676758766173  validation loss = 0.03853720799088478  accuracy = 58.01 %\n",
      "epoch 1/10 :  training loss = 1.9275793039798736  validation loss = 0.009117266163229942  accuracy = 61.29 %\n",
      "epoch 1/10 :  training loss = 1.9839680147171022  validation loss = 0.004747699946165085  accuracy = 63.23 %\n",
      "epoch 1/10 :  training loss = 1.926403056383133  validation loss = 0.0004811660328414291  accuracy = 63.35 %\n",
      "epoch 1/10 :  training loss = 1.9439350438117982  validation loss = 0.00041657889960333705  accuracy = 67.35 %\n",
      "epoch 1/10 :  training loss = 1.6301313549280168  validation loss = 0.00029205658938735723  accuracy = 67.48 %\n",
      "epoch 1/10 :  training loss = 1.6365929985046386  validation loss = 0.003725324524566531  accuracy = 68.81 %\n",
      "epoch 1/10 :  training loss = 1.7256964683532714  validation loss = 0.005739364307373762  accuracy = 64.81 %\n",
      "epoch 2/10 :  training loss = 0.8690883207321167  validation loss = 8.465651148981124e-07  accuracy = 71.97 %\n",
      "epoch 2/10 :  training loss = 1.4728148537874222  validation loss = 0.000499925808981061  accuracy = 69.05 %\n",
      "epoch 2/10 :  training loss = 1.27132916867733  validation loss = 2.7179688913747668e-05  accuracy = 71.72 %\n",
      "epoch 2/10 :  training loss = 1.3978591465950012  validation loss = 6.323792604234768e-06  accuracy = 70.51 %\n",
      "epoch 2/10 :  training loss = 1.3647114869952202  validation loss = 9.48547494772356e-06  accuracy = 71.12 %\n",
      "epoch 2/10 :  training loss = 1.491619001030922  validation loss = 2.6481527584110154e-06  accuracy = 71.72 %\n",
      "epoch 2/10 :  training loss = 1.530724386572838  validation loss = 0.0016262112185359001  accuracy = 70.87 %\n",
      "epoch 2/10 :  training loss = 1.414814031124115  validation loss = 0.00019397995492909104  accuracy = 73.30 %\n",
      "epoch 2/10 :  training loss = 1.4722824949026108  validation loss = 1.5624506133349314e-08  accuracy = 73.06 %\n",
      "epoch 2/10 :  training loss = 1.4485333231836557  validation loss = 4.258983494764834e-07  accuracy = 75.12 %\n",
      "epoch 2/10 :  training loss = 1.3676406195759774  validation loss = 2.3582137146149762e-05  accuracy = 75.12 %\n",
      "epoch 2/10 :  training loss = 1.265159372612834  validation loss = 6.981957994867116e-06  accuracy = 75.61 %\n",
      "epoch 2/10 :  training loss = 1.3363726314902307  validation loss = 6.307637079316919e-08  accuracy = 72.69 %\n",
      "epoch 2/10 :  training loss = 1.4245787042379379  validation loss = 3.7439605193867465e-07  accuracy = 73.67 %\n",
      "epoch 2/10 :  training loss = 1.48223817974329  validation loss = 5.427771725408093e-07  accuracy = 73.67 %\n",
      "epoch 2/10 :  training loss = 1.4637287381291388  validation loss = 2.951208273316297e-07  accuracy = 74.39 %\n",
      "epoch 3/10 :  training loss = 0.28263779044151305  validation loss = 8.860966772772372e-06  accuracy = 76.70 %\n",
      "epoch 3/10 :  training loss = 1.165654810667038  validation loss = 8.999685633170884e-06  accuracy = 76.21 %\n",
      "epoch 3/10 :  training loss = 1.1274374070763589  validation loss = 9.753594895300921e-06  accuracy = 76.33 %\n",
      "epoch 3/10 :  training loss = 1.086150253303349  validation loss = 1.75776835931174e-06  accuracy = 76.94 %\n",
      "epoch 3/10 :  training loss = 1.1602646541595458  validation loss = 2.0732099983433727e-06  accuracy = 75.73 %\n",
      "epoch 3/10 :  training loss = 1.492789391875267  validation loss = 0.00011488263407954946  accuracy = 76.21 %\n",
      "epoch 3/10 :  training loss = 1.4363122968375683  validation loss = 0.03169570118188858  accuracy = 76.33 %\n",
      "epoch 3/10 :  training loss = 1.148671374320984  validation loss = 1.0821344886835504e-07  accuracy = 77.91 %\n",
      "epoch 3/10 :  training loss = 1.2565064436197282  validation loss = 9.837585679406402e-08  accuracy = 76.46 %\n",
      "epoch 3/10 :  training loss = 1.3115331685543061  validation loss = 1.535035494271142e-06  accuracy = 78.76 %\n",
      "epoch 3/10 :  training loss = 1.354859595298767  validation loss = 2.0716512949547905e-07  accuracy = 77.43 %\n",
      "epoch 3/10 :  training loss = 1.2331435199081897  validation loss = 3.1294089239963796e-06  accuracy = 80.10 %\n",
      "epoch 3/10 :  training loss = 1.4222065532207488  validation loss = 8.083637226263818e-07  accuracy = 78.28 %\n",
      "epoch 3/10 :  training loss = 1.2674966298043728  validation loss = 0.00015574932331219316  accuracy = 79.00 %\n",
      "epoch 3/10 :  training loss = 1.3175531208515168  validation loss = 1.783835432433989e-05  accuracy = 75.97 %\n",
      "epoch 3/10 :  training loss = 1.0914469799399376  validation loss = 3.5299773770702814e-08  accuracy = 79.61 %\n",
      "epoch 3/10 :  training loss = 1.3721322894096375  validation loss = 1.7156748072011396e-05  accuracy = 76.58 %\n",
      "epoch 4/10 :  training loss = 0.9652360540628433  validation loss = 5.965959530840337e-07  accuracy = 75.24 %\n",
      "epoch 4/10 :  training loss = 1.4019858607649802  validation loss = 1.3574114063885645e-06  accuracy = 78.40 %\n",
      "epoch 4/10 :  training loss = 1.2007222358882428  validation loss = 1.3609558209282113e-06  accuracy = 79.13 %\n",
      "epoch 4/10 :  training loss = 1.075300734275952  validation loss = 3.472008529570303e-07  accuracy = 78.40 %\n",
      "epoch 4/10 :  training loss = 1.4289129722118377  validation loss = 7.425546755257528e-06  accuracy = 78.03 %\n",
      "epoch 4/10 :  training loss = 1.1213243721425534  validation loss = 2.0774547238033847e-07  accuracy = 77.67 %\n",
      "epoch 4/10 :  training loss = 1.3053322371840477  validation loss = 2.4767194872765685e-07  accuracy = 82.52 %\n",
      "epoch 4/10 :  training loss = 1.213870671093464  validation loss = 0.0005336753092706203  accuracy = 79.85 %\n",
      "epoch 4/10 :  training loss = 1.2848254787921904  validation loss = 7.15192363713868e-05  accuracy = 77.31 %\n",
      "epoch 4/10 :  training loss = 1.293637490272522  validation loss = 7.587102300021797e-05  accuracy = 79.37 %\n",
      "epoch 4/10 :  training loss = 1.104559275507927  validation loss = 4.618545972334687e-06  accuracy = 75.49 %\n",
      "epoch 4/10 :  training loss = 1.1410602805018426  validation loss = 7.074430322973058e-05  accuracy = 76.46 %\n",
      "epoch 4/10 :  training loss = 1.2455457207560539  validation loss = 9.119534070123336e-07  accuracy = 78.03 %\n",
      "epoch 4/10 :  training loss = 1.1140287121012806  validation loss = 1.0085969961437513e-06  accuracy = 80.58 %\n",
      "epoch 4/10 :  training loss = 1.1350579831749201  validation loss = 2.7280600988888182e-05  accuracy = 78.76 %\n",
      "epoch 4/10 :  training loss = 1.0523794648051261  validation loss = 0.0013582675019279122  accuracy = 80.22 %\n",
      "epoch 5/10 :  training loss = 0.5659619718790054  validation loss = 0.00016052988939918578  accuracy = 77.55 %\n",
      "epoch 5/10 :  training loss = 1.1164560952782632  validation loss = 2.8934286078197147e-09  accuracy = 75.85 %\n",
      "epoch 5/10 :  training loss = 1.2410460078716279  validation loss = 0.00032378602190874517  accuracy = 78.16 %\n",
      "epoch 5/10 :  training loss = 0.834232050254941  validation loss = 0.000661901431158185  accuracy = 78.76 %\n",
      "epoch 5/10 :  training loss = 0.9723788714408874  validation loss = 0.019397925585508347  accuracy = 79.37 %\n",
      "epoch 5/10 :  training loss = 1.071092877984047  validation loss = 3.213381569366902e-05  accuracy = 81.19 %\n",
      "epoch 5/10 :  training loss = 1.1228289370238782  validation loss = 2.721378223213833e-06  accuracy = 82.16 %\n",
      "epoch 5/10 :  training loss = 1.2051594893634319  validation loss = 4.515375985647552e-05  accuracy = 76.82 %\n",
      "epoch 5/10 :  training loss = 1.1110936027765275  validation loss = 1.998369953071233e-06  accuracy = 79.37 %\n",
      "epoch 5/10 :  training loss = 1.1634539175219833  validation loss = 1.0664452929631807e-05  accuracy = 81.07 %\n",
      "epoch 5/10 :  training loss = 1.1254899623990058  validation loss = 5.236829565546941e-07  accuracy = 80.34 %\n",
      "epoch 5/10 :  training loss = 1.1260572525113821  validation loss = 1.388600139762275e-05  accuracy = 80.83 %\n",
      "epoch 5/10 :  training loss = 1.3146604070067405  validation loss = 0.0023231585510075092  accuracy = 80.95 %\n",
      "epoch 5/10 :  training loss = 1.0279416129924357  validation loss = 1.4508173990179785e-05  accuracy = 80.10 %\n",
      "epoch 5/10 :  training loss = 1.070964157022536  validation loss = 3.4632396364031592e-06  accuracy = 82.89 %\n",
      "epoch 5/10 :  training loss = 1.1783697381615639  validation loss = 0.00012630547280423343  accuracy = 78.88 %\n",
      "epoch 6/10 :  training loss = 0.12260621964931488  validation loss = 4.8568231250101235e-06  accuracy = 82.65 %\n",
      "epoch 6/10 :  training loss = 1.003092297539115  validation loss = 2.3046206933940994e-06  accuracy = 80.10 %\n",
      "epoch 6/10 :  training loss = 1.0928877910971642  validation loss = 3.026437127573445e-07  accuracy = 80.10 %\n",
      "epoch 6/10 :  training loss = 1.041833119392395  validation loss = 4.8609376079866706e-08  accuracy = 78.88 %\n",
      "epoch 6/10 :  training loss = 1.1193852062523364  validation loss = 9.507224945082271e-07  accuracy = 79.61 %\n",
      "epoch 6/10 :  training loss = 1.0089473569393157  validation loss = 1.09475047338492e-06  accuracy = 76.70 %\n",
      "epoch 6/10 :  training loss = 1.1466352423280477  validation loss = 6.365127092067269e-07  accuracy = 78.88 %\n",
      "epoch 6/10 :  training loss = 1.1426474799215793  validation loss = 7.701435606577434e-06  accuracy = 81.43 %\n",
      "epoch 6/10 :  training loss = 0.913920856937766  validation loss = 4.6466252001664543e-07  accuracy = 80.58 %\n",
      "epoch 6/10 :  training loss = 1.090719411969185  validation loss = 0.0013762024464085698  accuracy = 82.52 %\n",
      "epoch 6/10 :  training loss = 1.0432331838458777  validation loss = 3.819311800157266e-08  accuracy = 80.46 %\n",
      "epoch 6/10 :  training loss = 1.1067588862776756  validation loss = 4.195292433450959e-07  accuracy = 81.67 %\n",
      "epoch 6/10 :  training loss = 1.0866100503504277  validation loss = 1.8806929347192636e-07  accuracy = 82.52 %\n",
      "epoch 6/10 :  training loss = 1.1100994165986777  validation loss = 2.9512889909710793e-08  accuracy = 80.70 %\n",
      "epoch 6/10 :  training loss = 1.1289262763410806  validation loss = 3.053936598007567e-05  accuracy = 82.40 %\n",
      "epoch 6/10 :  training loss = 1.1696953195333482  validation loss = 1.64040393428877e-05  accuracy = 79.98 %\n",
      "epoch 6/10 :  training loss = 1.0195870950818062  validation loss = 0.0035129550378769636  accuracy = 81.55 %\n",
      "epoch 7/10 :  training loss = 0.6269301520287991  validation loss = 7.82328925197362e-07  accuracy = 80.46 %\n",
      "epoch 7/10 :  training loss = 0.9883927112165839  validation loss = 2.5494073270238005e-05  accuracy = 79.37 %\n",
      "epoch 7/10 :  training loss = 0.8090484992042184  validation loss = 4.629469430028621e-08  accuracy = 82.65 %\n",
      "epoch 7/10 :  training loss = 0.9502998501062393  validation loss = 5.0924171546284924e-08  accuracy = 80.34 %\n",
      "epoch 7/10 :  training loss = 1.1696135684847833  validation loss = 0.0  accuracy = 80.58 %\n",
      "epoch 7/10 :  training loss = 1.0229768854752184  validation loss = 0.0  accuracy = 79.25 %\n",
      "epoch 7/10 :  training loss = 0.8786808321624995  validation loss = 6.365540095032429e-09  accuracy = 79.00 %\n",
      "epoch 7/10 :  training loss = 0.9205293325334787  validation loss = 0.0  accuracy = 82.28 %\n",
      "epoch 7/10 :  training loss = 0.9727118459995836  validation loss = 1.1842258572869468e-05  accuracy = 80.10 %\n",
      "epoch 7/10 :  training loss = 0.9974998253583908  validation loss = 4.5137287685292904e-08  accuracy = 81.31 %\n",
      "epoch 7/10 :  training loss = 1.1101456667482852  validation loss = 2.3726062892137634e-08  accuracy = 81.92 %\n",
      "epoch 7/10 :  training loss = 1.0861703466251493  validation loss = 3.458730361671769e-06  accuracy = 81.55 %\n",
      "epoch 7/10 :  training loss = 1.101165840998292  validation loss = 7.754331932119385e-08  accuracy = 81.55 %\n",
      "epoch 7/10 :  training loss = 0.9069677519798279  validation loss = 1.7068232409656048e-06  accuracy = 82.77 %\n",
      "epoch 7/10 :  training loss = 1.0470323826372623  validation loss = 1.738061428113724e-06  accuracy = 83.37 %\n",
      "epoch 7/10 :  training loss = 1.0858802244067192  validation loss = 5.786858214840152e-10  accuracy = 84.59 %\n",
      "epoch 8/10 :  training loss = 0.2682994666695595  validation loss = 0.0  accuracy = 83.13 %\n",
      "epoch 8/10 :  training loss = 1.022257210984826  validation loss = 0.0  accuracy = 81.43 %\n",
      "epoch 8/10 :  training loss = 0.9620774052850902  validation loss = 1.7360574089408942e-09  accuracy = 83.13 %\n",
      "epoch 8/10 :  training loss = 0.927508402094245  validation loss = 3.4721141517479737e-09  accuracy = 81.19 %\n",
      "epoch 8/10 :  training loss = 0.9400550987571478  validation loss = 3.2406301642140534e-08  accuracy = 80.34 %\n",
      "epoch 8/10 :  training loss = 0.9201466984674335  validation loss = 4.629484795515282e-09  accuracy = 81.43 %\n",
      "epoch 8/10 :  training loss = 1.2066538225859404  validation loss = 2.2588008505408652e-05  accuracy = 81.92 %\n",
      "epoch 8/10 :  training loss = 1.1425030192360282  validation loss = 2.7198193919275582e-08  accuracy = 82.04 %\n",
      "epoch 8/10 :  training loss = 1.025556856021285  validation loss = 5.207899107517733e-07  accuracy = 81.43 %\n",
      "epoch 8/10 :  training loss = 1.053018452944234  validation loss = 0.001264156075194478  accuracy = 83.01 %\n",
      "epoch 8/10 :  training loss = 1.2379858618974686  validation loss = 1.1705642464221455e-06  accuracy = 82.16 %\n",
      "epoch 8/10 :  training loss = 1.1036559572815896  validation loss = 5.3238895958429566e-08  accuracy = 81.92 %\n",
      "epoch 8/10 :  training loss = 0.9558402163535357  validation loss = 5.786858214840152e-10  accuracy = 81.67 %\n",
      "epoch 8/10 :  training loss = 1.1246005627512932  validation loss = 4.6351573246283806e-07  accuracy = 80.22 %\n",
      "epoch 8/10 :  training loss = 1.1916597959399224  validation loss = 6.0240176935622e-06  accuracy = 79.00 %\n",
      "epoch 8/10 :  training loss = 0.8662215526343789  validation loss = 4.1665266792279e-08  accuracy = 82.77 %\n",
      "epoch 8/10 :  training loss = 1.0619633197784424  validation loss = 9.484319889452308e-06  accuracy = 81.67 %\n",
      "epoch 9/10 :  training loss = 1.0158022368326782  validation loss = 7.256400067490176e-07  accuracy = 82.65 %\n",
      "epoch 9/10 :  training loss = 0.8473283589631319  validation loss = 7.933167580631562e-07  accuracy = 82.65 %\n",
      "epoch 9/10 :  training loss = 0.7819419517181814  validation loss = 2.352421006435179e-06  accuracy = 83.37 %\n",
      "epoch 9/10 :  training loss = 0.7982320625707507  validation loss = 7.464993956318722e-08  accuracy = 83.25 %\n",
      "epoch 9/10 :  training loss = 1.082716417387128  validation loss = 4.031705429952126e-06  accuracy = 81.43 %\n",
      "epoch 9/10 :  training loss = 1.0168718151282519  validation loss = 1.7196628050442087e-06  accuracy = 82.16 %\n",
      "epoch 9/10 :  training loss = 1.0162932833842933  validation loss = 1.1403766620787792e-05  accuracy = 82.16 %\n",
      "epoch 9/10 :  training loss = 0.9598651415109635  validation loss = 0.0003281057870481163  accuracy = 84.22 %\n",
      "epoch 9/10 :  training loss = 1.0806124657392502  validation loss = 3.226917397114448e-05  accuracy = 83.74 %\n",
      "epoch 9/10 :  training loss = 0.9362669572606683  validation loss = 0.0003941398172173649  accuracy = 80.95 %\n",
      "epoch 9/10 :  training loss = 1.3409311923384666  validation loss = 0.0004062767548020929  accuracy = 82.65 %\n",
      "epoch 9/10 :  training loss = 1.006486986335367  validation loss = 6.134031593774125e-08  accuracy = 82.52 %\n",
      "epoch 9/10 :  training loss = 0.813160528796725  validation loss = 1.0982346339005744e-06  accuracy = 83.62 %\n",
      "epoch 9/10 :  training loss = 1.0386860735947265  validation loss = 2.912349737016484e-05  accuracy = 81.43 %\n",
      "epoch 9/10 :  training loss = 1.0745287925004958  validation loss = 1.0943655070150271e-05  accuracy = 82.04 %\n",
      "epoch 9/10 :  training loss = 1.1060570948943496  validation loss = 1.2067905117874034e-05  accuracy = 82.65 %\n",
      "epoch 10/10 :  training loss = 0.615592017294839  validation loss = 0.0007198804523795843  accuracy = 82.16 %\n",
      "epoch 10/10 :  training loss = 0.578506870418787  validation loss = 7.5193624979874585e-06  accuracy = 85.92 %\n",
      "epoch 10/10 :  training loss = 0.826407665386796  validation loss = 7.227266678455635e-07  accuracy = 84.59 %\n",
      "epoch 10/10 :  training loss = 0.9174942184984684  validation loss = 5.200285613682354e-06  accuracy = 83.01 %\n",
      "epoch 10/10 :  training loss = 0.8269245454668999  validation loss = 1.0416248841238485e-07  accuracy = 84.47 %\n",
      "epoch 10/10 :  training loss = 0.9245498015731574  validation loss = 6.6804045673052315e-06  accuracy = 83.86 %\n",
      "epoch 10/10 :  training loss = 0.9150596329197288  validation loss = 6.9895827436994296e-06  accuracy = 83.25 %\n",
      "epoch 10/10 :  training loss = 0.9454047830775381  validation loss = 7.014978109509684e-06  accuracy = 84.71 %\n",
      "epoch 10/10 :  training loss = 0.8950240405648947  validation loss = 7.794275234118686e-07  accuracy = 83.01 %\n",
      "epoch 10/10 :  training loss = 0.9621562559809536  validation loss = 1.0358366608897995e-07  accuracy = 83.01 %\n",
      "epoch 10/10 :  training loss = 1.0263572573848068  validation loss = 1.446712527553018e-08  accuracy = 82.52 %\n",
      "epoch 10/10 :  training loss = 1.1085469894856215  validation loss = 4.039143277623225e-07  accuracy = 82.04 %\n",
      "epoch 10/10 :  training loss = 1.02161589063704  validation loss = 4.056441014199663e-07  accuracy = 82.16 %\n",
      "epoch 10/10 :  training loss = 0.9116773330047727  validation loss = 4.629486127782911e-09  accuracy = 83.01 %\n",
      "epoch 10/10 :  training loss = 1.0310093484073877  validation loss = 5.786858214840152e-10  accuracy = 83.25 %\n",
      "epoch 10/10 :  training loss = 1.0235733337514104  validation loss = 0.0  accuracy = 81.31 %\n"
     ]
    }
   ],
   "source": [
    "train_model_wrapper(arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Processing Image for Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    img = Image.open(image)\n",
    "    img_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(224),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485,0.456,0.406],\n",
    "                                                           [0.229,0.224,0.225])])\n",
    "    return img_transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model Parameters Saved after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(saved_model):\n",
    "    checkpoint_provided = torch.load(saved_model)\n",
    "    if checkpoint_provided['arch'] == 'vgg':\n",
    "        model = models.vgg16()        \n",
    "    elif checkpoint_provided['arch'] == 'densenet':\n",
    "        model = models.densenet121()\n",
    "    num_features = model.classifier[0].in_features\n",
    "    from collections import OrderedDict\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(num_features, 512)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('drpot', nn.Dropout(p=0.5)),\n",
    "                          ('hidden', nn.Linear(512, hidden_units)),                        \n",
    "                          ('fc2', nn.Linear(hidden_units, 102)),\n",
    "                          ('output', nn.LogSoftmax(dim=1)),\n",
    "                          ]))\n",
    "    model.classifier = classifier\n",
    "    model.load_state_dict(checkpoint_provided['state_dict'])\n",
    "    if gpu:\n",
    "        if use_gpu:\n",
    "            model = model.cuda()\n",
    "            print (\"Using GPU\")\n",
    "        else:\n",
    "            print(\"Using CPU\")\n",
    "    class_to_idx = checkpoint_provided['class_to_idx']\n",
    "    idx_to_class = { v : k for k,v in class_to_idx.items()}\n",
    "    return model, class_to_idx, idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function To Show 5 Images with the Top Prediction Probabiliities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, class_to_idx, idx_to_class, cat_to_name, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    image = process_image(image_path)\n",
    "    image.unsqueeze_(0)\n",
    "    if use_gpu and gpu:\n",
    "        model = model.cuda()    \n",
    "    model.eval()\n",
    "    if use_gpu and gpu:\n",
    "        output = model.forward(image.cuda())\n",
    "    else:\n",
    "        output = model.forward(image)\n",
    "    ps = torch.exp(output.cpu())#.data.numpy()[0]\n",
    "    top_ps, top_labels = ps.topk(topk, dim=1)\n",
    "    top_ps = top_ps.detach().numpy().tolist()[0]\n",
    "    top_flowers = [cat_to_name[idx_to_class[x]] for x in top_labels.detach().numpy().tolist()[0]]\n",
    "    return top_ps,top_flowers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the Args to be Passed to the above Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = True\n",
    "hidden_units = 100\n",
    "saved_model = \"model.pth\"\n",
    "topk = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ![pink primrose](flowers/test/1/image_06743.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pink primrose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Predicted Classes:  ['pink primrose', 'hibiscus', 'watercress', 'mallow', 'tree mallow']\n",
      "Predicted Probability:  [0.9598137140274048, 0.02773149497807026, 0.010170906782150269, 0.0016894926084205508, 0.0002148736675735563]\n",
      "predicted class pink primrose with an accuracy of 95.98137140274048%\n"
     ]
    }
   ],
   "source": [
    "image_path = \"flowers/test/1/image_06743.jpg\" \n",
    "\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "        cat_to_name = json.load(f)\n",
    "\n",
    "model, class_to_idx, idx_to_class = load_checkpoint(saved_model)\n",
    "top_probability, top_predictions = predict(image_path, model, class_to_idx, idx_to_class, cat_to_name, topk=topk)\n",
    "\n",
    "print('Predicted Classes: ', top_predictions)\n",
    "print('Predicted Probability: ', top_probability)\n",
    "print ('predicted class {} with an accuracy of {}%'.format(top_predictions[0],top_probability[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ![globe thistle](flowers/test/10/image_07090.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### globe thistle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Predicted Classes:  ['globe thistle', 'artichoke', 'common dandelion', 'king protea', 'pincushion flower']\n",
      "Predicted Probability:  [0.9998131394386292, 0.00017802120419219136, 7.51867582948762e-06, 1.2235684607730946e-06, 9.893702923591263e-08]\n",
      "predicted class globe thistle with an accuracy of 99.98131394386292%\n"
     ]
    }
   ],
   "source": [
    "image_path = \"flowers/test/10/image_07090.jpg\" \n",
    "\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "        cat_to_name = json.load(f)\n",
    "\n",
    "model, class_to_idx, idx_to_class = load_checkpoint(saved_model)\n",
    "top_probability, top_predictions = predict(image_path, model, class_to_idx, idx_to_class, cat_to_name, topk=topk)\n",
    "\n",
    "print('Predicted Classes: ', top_predictions)\n",
    "print('Predicted Probability: ', top_probability)\n",
    "print ('predicted class {} with an accuracy of {}%'.format(top_predictions[0],top_probability[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ![arum lily](flowers/test/20/image_04912.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### arum lily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Predicted Classes:  ['giant white arum lily', 'moon orchid', 'frangipani', 'ruby-lipped cattleya', 'thorn apple']\n",
      "Predicted Probability:  [0.9993442893028259, 0.00041622499702498317, 0.00014414452016353607, 3.6253604775993153e-05, 3.0203471396816894e-05]\n",
      "predicted class giant white arum lily with an accuracy of 99.93442893028259%\n"
     ]
    }
   ],
   "source": [
    "image_path = \"flowers/test/20/image_04912.jpg\" \n",
    "\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "        cat_to_name = json.load(f)\n",
    "\n",
    "model, class_to_idx, idx_to_class = load_checkpoint(saved_model)\n",
    "top_probability, top_predictions = predict(image_path, model, class_to_idx, idx_to_class, cat_to_name, topk=topk)\n",
    "\n",
    "print('Predicted Classes: ', top_predictions)\n",
    "print('Predicted Probability: ', top_probability)\n",
    "print ('predicted class {} with an accuracy of {}%'.format(top_predictions[0],top_probability[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ![sweet william](flowers/test/30/image_03466.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sweet william"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Predicted Classes:  ['sweet william', 'barbeton daisy', 'tree mallow', 'carnation', 'petunia']\n",
      "Predicted Probability:  [0.9739780426025391, 0.012665963731706142, 0.010556580498814583, 0.0008589239441789687, 0.000856724102050066]\n",
      "predicted class sweet william with an accuracy of 97.3978042602539%\n"
     ]
    }
   ],
   "source": [
    "image_path = \"flowers/test/30/image_03466.jpg\" \n",
    "\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "        cat_to_name = json.load(f)\n",
    "\n",
    "model, class_to_idx, idx_to_class = load_checkpoint(saved_model)\n",
    "top_probability, top_predictions = predict(image_path, model, class_to_idx, idx_to_class, cat_to_name, topk=topk)\n",
    "\n",
    "print('Predicted Classes: ', top_predictions)\n",
    "print('Predicted Probability: ', top_probability)\n",
    "print ('predicted class {} with an accuracy of {}%'.format(top_predictions[0],top_probability[0]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ![lenten rose](flowers/test/40/image_04563.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lenten rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Predicted Classes:  ['lenten rose', 'columbine', 'hippeastrum', 'camellia', 'mallow']\n",
      "Predicted Probability:  [0.9578976631164551, 0.03748911991715431, 0.004568890202790499, 3.2806296076159924e-05, 6.57527152725379e-06]\n",
      "predicted class lenten rose with an accuracy of 95.78976631164551%\n"
     ]
    }
   ],
   "source": [
    "image_path = \"flowers/test/40/image_04563.jpg\" \n",
    "\n",
    "with open('cat_to_name.json', 'r') as f:\n",
    "        cat_to_name = json.load(f)\n",
    "\n",
    "model, class_to_idx, idx_to_class = load_checkpoint(saved_model)\n",
    "top_probability, top_predictions = predict(image_path, model, class_to_idx, idx_to_class, cat_to_name, topk=topk)\n",
    "\n",
    "print('Predicted Classes: ', top_predictions)\n",
    "print('Predicted Probability: ', top_probability)\n",
    "print ('predicted class {} with an accuracy of {}%'.format(top_predictions[0],top_probability[0]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4776eb30a046ab6d0c3e65f56044d961f6878ebd8f8cbdf1a791b04d1d0b3d44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
